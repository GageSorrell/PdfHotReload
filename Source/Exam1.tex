\documentclass{article}
\usepackage[OT1]{fontenc}
\usepackage{setspace}
\usepackage{geometry}

% Added for \mathbb, blackboard letters.
\usepackage{amsmath}
\usepackage{amsfonts}

% Added for \coloneqq, a typeset form of $:=$.
\usepackage{mathtools}

% Added for bold letters for representing vectors.
\usepackage{bm}

\geometry{
    letterpaper,
    total={7.5in, 9.5in}
}
\usepackage{graphicx}
\usepackage{titling}

\setlength{\headheight}{12.5pt}

\title{STAT 517 -- Exam 1}
\author{Gage Sorrell}
\date{February 22, 2023}
 
\usepackage{fancyhdr}
\fancypagestyle{plain}{%  the preset of fancyhdr
    \fancyhf{} % clear all header and footer fields
    % \fancyfoot[R]{\includegraphics[width=2cm]{KULEUVEN_GENT_RGB_LOGO.png}}
    % \fancyfoot[L]{January 13, 2023}
    \fancyhead[L]{Exam 1}
    \fancyhead[R]{Gage Sorrell}
}
\makeatletter
\def\@maketitle{%
    \newpage
    \null
    \vskip 1em%
    \begin{center}%
    \let \footnote \thanks
        {\LARGE \@title \par}%
        \vskip 1em%
        %{\large \@date}%
        \end{center}%
        \par
        \vskip 1em}
    \makeatother

\begin{document}

\maketitle

\noindent\begin{tabular}{@{}ll}
    Student & Gage Sorrell\\
    Professor &  Dr. Yvonne Zubovich, PhD\\
\end{tabular}

\doublespacing

\section*{Problem \#1}

% The prior p.f., $\zeta(\theta)$ maps observable values of $\theta$ to the probability of observing each possible value of $\theta$.
It is often based on relevant subject matter and previous studies.

The posterior p.f., $\zeta(\theta | \mathbf{X})$, maps $\theta$ as before, but in a way that accounts for newly-observed data, which better informs the statistician regarding the relevant phenomenon.
By \textbf{Theorem 7.2.1}, it is shown that the posterior p.f. is proportionate to the prior p.f.

The prior p.f. gives the statistician a rough ``starting point'' to understand the relevant phenomenon, and the posterior p.f. allows the statistician to reach conclusions based on the observed values.

\section*{Problem \#2}

An estimator is a function that produces an estimate, which is an approximation of the true value of the population parameter.
The estimator function accepts sample data as input, and can also be a function of unknown parameters.

\section*{Problem \#3}

\subsection*{Part A}

The expectation of Karl's prior is lower than the face-to-face mean score, meaning that Karl believes that online courses are associated with lower scores.
Ronald's expectation is higher, meaning that he believes that online courses are associated with higher scores, \textit{i.e.}, he believes that online courses will be associated with higher scores.

Ronald's prior has a smaller standard deviation than Karl's, $\sqrt{8}$ versus $\sqrt{50}$, meaning that he is more confident that scores will be closer to his expectation than Karl is \textit{wrt} his expectation of $65$.

\subsection*{Part B}

Karl believes that students will perform worse when taking courses online, and Ronald believes that they will perform better.
This is modeled in the expectations: $65 < 70$ for Karl, and $80 > 70$ for Ronald.
Considering only the mean, one can assume that Karl expects students to only perform slightly worse, whereas Ronald believes that students will perform much better, \textit{i.e.}, by one letter grade.

Taking the variance into account as well shows that Karl expects fewer students to score close to $65$ than Ronald expects students to score close to $80$, with respective variances $50$ and $8$.

\section*{Problem \#4}

TODO reword this section.

It is assumed that significant risk of damaging the elevator by overloading it is equally unfavorable as it would be to underload the elevator and forego productivity.
Thus, for the loss to be minimized, it should be reflected that the cost of underestimating be roughly equal to the cost of overestimating.

If we choose $c = 1$, then the costs of underestimating and overestimating are equal when $A - \theta = \theta - A \Longrightarrow A = \theta$.
This gives a linear relationship between loss and $A - \theta$.

If we choose $c = 2$, then the cost of underestimating is twice that of overestimating.
This is shown by $A - \theta = 2(\theta - A) \Longrightarrow A = \frac{2}{3}\theta$, and for underestimating, $\theta - A = \theta - \frac{2}{3}\theta = \frac{\theta}{3}$.

If we choose $c = \frac{1}{2}$, then the cost of overestimating is twice that of underestimating.
This is shown by $A - \theta = \frac{1}{2}(\theta - A) \Longrightarrow A = \frac{3}{2}\theta$, and for underestimating, $\theta - A = \theta \frac{1}{3}\theta = \frac{2}{3}\theta$.

Thus the best choice is $c = 1$.

\section*{Problem \#5}

The statement is not accurate; the MLE of $\theta$ does not represent the probability of a particular value of $\theta$.
Instead, the MLE of $\theta$ maximizes the likelihood function, which represents the probability of observing the given data as a function of $\theta$.

While the concepts in the supposition are closely related, the \textit{causation} implied by the supposition does not reflect how the concepts work.

\section*{Problem \#6}

Recall that the variance of $\theta$ is $\operatorname{Var}X = \theta(1 - \theta)$ for a Bernoulli distribution.
Using the Method of Moments estimator of $\operatorname{Var}X$, we find the first moment,
\[
    \operatorname{E}(X) = \theta \overset{\text{MoM}}{\Longrightarrow} \theta = \overline{X}_n \Longrightarrow \hat{\theta} = \overline{X}_n
\]

Now we find the second moment, which allows us to solve for the variance,
\[
    \operatorname{E}(X^2) = \operatorname{Var}X + \operatorname{E}^2(X) = \operatorname{Var}X + \theta^2 \Longrightarrow \operatorname{Var}X = \operatorname{E}(X^2) - \operatorname{E}^2(X) = \theta - \theta^2 = \boxed{\theta(1 - \theta) = \operatorname{Var}X}
\]

We plug in the observed data, which allows us to solve for the variance,
\[
    \hat{\theta} = \overline{X}_n = \frac{160}{200} = 0.8 \Longrightarrow \operatorname{Var}X = \hat{\theta}(1 - \hat{\theta}) = 0.8(1 - 0.8) = \boxed{0.16}
\]

\section*{Problem \#7}

% \[
%     f(x|\theta) =
%     \begin{cases}
%         \frac{1}{2\theta} & \text{for } x \in (0, \theta) \\
%         0 & \text{otherwise}
%     \end{cases}
% \]

TODO add textbook source here
Consider that $\operatorname{E}(X) = \int_{-\infty}^\infty x f(x | \theta)dx$.
We can restrict the interval to where the p.f. is non-zero,
\[
    \operatorname{E}(X) = \int_0^\theta \frac{x}{2\theta}dx = \cdots = \frac{\theta}{3}
\]

Now that we have the population mean, we set the sample mean equal to the population mean,
\[
    n^{-1}\sum_{i = 1}^n X_i = \frac{\hat{\theta}}{3} \Longrightarrow \hat{\theta} = \frac{3}{n}\sum_{i = 1}^n X_i
\]

Thus the method of moments estimator is found.

\section*{Problem \#8}

\subsection*{Part A}

% \[
%     f(x|\theta) =
%     \begin{cases}   
%         \theta^{-1} & \text{for } x \in (0, \theta) \\
%         0 & \text{otherwise}
%     \end{cases}
% \]

% \[
%     \zeta(\theta) =
%     \begin{cases}
%         \theta e^{-\theta} & \text{for } \theta > 0 \\
%         0 & \text{otherwise}
%     \end{cases}
% \]

TODO cite this
Recall that by Bayes' theorem,
\[
    \zeta(\theta | \bm{X}) \propto f(\bm{X}|\theta)\zeta(\theta) \Longrightarrow \zeta(\theta | \bm{X}) \propto \theta^{-1}\theta e^{-\theta} = e^{-\theta}
\]

TODO Cite this
Thus by ..., the posterior p.f. is
\[
    \zeta(\theta | \bm{X}) =
    \begin{cases}
        \theta e^{-\theta} & \text{for } \theta > 0\\
        0 & \text{otherwise}
    \end{cases}
\]

\subsection*{Part B}

TODO Parts B and C likely have simplified versions in the text.

\begin{align*}
    \hat{\theta} &= \operatorname{E}(\theta | X) \\
    &= \int_{0}^{\infty} \theta \cdot \zeta(\theta | X) d\theta \\
    &= \int_{0}^{\infty} \theta \cdot \frac{\theta e^{-\theta x}}{1 - e^{-\theta x}} d\theta \\
    &= \boxed{\int_{0}^{\infty} \frac{\theta^2 e^{-\theta x}}{1 - e^{-\theta x}} d\theta}
\end{align*}

\subsection*{Part C}

% \[
%     f(x | \theta) =
%     \begin{cases}
%         \frac{\theta}{x^{\theta + 1}} & \text{for } x \geq 1\\
%         0 &\text{otherwise}
%     \end{cases}
% \]

TODO cite this
The likelihood function is given by
\begin{align*}
    \operatorname{L}(\theta | \bm{X}) &= \prod_{i = 1}^n f(X_i | \theta)\\
    &= \prod_{i = 1}^n \frac{\theta}{X_i^{\theta + 1}}\\
    &= \theta^n \Bigg(\prod_{i = 1}^n X_i\Bigg)^{-\theta - 1}
\end{align*}

Taking the natural logarithm of the likelihood function gives
\[
    \ln{L(\theta | \bm{X})} = n\ln{\theta} - (\theta + 1)\sum_{i = 1}^n \ln{X_i}
\]

And the derivative, set to zero
\[
    \operatorname{ln}_\theta \theta | \bm{X} = \frac{n}{\theta} - \sum_{i = 1}^n \ln{X_i} = 0
\]
\[
    0 = \frac{n}{\theta} - \sum_{i = 1}^n \ln{X_i} \Longrightarrow \frac{n}{\hat{\theta}} = \sum_{i = 1}^n \ln{X_i} \Longrightarrow \boxed{\hat{\theta} = \frac{n}{\sum_{i = 1}^n \ln{X_i}}}
\]

\subsection*{Part B}

TODO flesh out solving the integral
The first moment is given by
\begin{align*}
    \operatorname{E}(X) &= \int_1^\infty xf(x|\theta)dx\\
    &= \int_1^\infty x\frac{\theta}{x^{\theta + 1}} dx\\
    &= \theta \int_1^\infty \frac{1}{x^\theta}dx\\
    &= \frac{\theta}{\theta - 1}
\end{align*}

We set this equal to the first sample moment,
\[
    \overline{X}_n = \frac{\theta}{\theta - 1} \Longrightarrow \overline{X}_n(\theta - 1) = \theta \Longrightarrow \overline{X}_n \theta - \overline{X}_n = \theta \Longrightarrow \overline{X}_n \theta = \theta + \overline{X}_n \Longrightarrow \theta(\overline{X}_n - 1) = \overline{X}_n \Longrightarrow \boxed{\hat{\theta} = \frac{\overline{X}_n}{\overline{X}_n - 1}}
\]



\end{document}